!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"
import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # Property used to format output tables better
spark
from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType,DoubleType

customSchema = StructType([
    StructField("user_id", StringType(), True),
    StructField("account_id", StringType(), True),
    StructField("counterparty_id", StringType(), True),
    StructField("transaction_type", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("Date", StringType(), True)
])
#Reading the CSV File
df = spark.read.csv('/Transaction.csv', header=True, sep=",",schema=customSchema)
group_cols = ["user_id", "counterparty_id"]

#Aggregation
df.groupBy('user_id', 'counterparty_id').sum('amount').show()
%pip install pyspark pytest pytest-spark

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

# Defining your custom schema
customSchema = StructType([
    StructField("user_id", StringType(), True),
    StructField("account_id", StringType(), True),
    StructField("counterparty_id", StringType(), True),
    StructField("transaction_type", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("Date", StringType(), True)
])

# Loading the Df
df = spark.read.csv('/Transaction.csv', header=True, sep=",", schema=customSchema)

# Defining the function to test
def Get_Transaction(df):
    #calculation logic
    group_cols = ["user_id", "counterparty_id"]
    result_df = df.groupBy(*group_cols).sum('amount')
    return result_df

# Calling the function to calculate the max amount
result_df = Get_Transaction(df)

result_df.show()

import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

# Spark session for testing
spark_test = SparkSession.builder.appName("test").getOrCreate()

# Defining the test function
def test_Get_Transaction():
    # Create a test DataFrame with dummy happy data criteria
    test_data = [("u1", "u1a1", "u3", "outgoing", 100.0, "1/1/2023"),
                 ("u1", "u1a2", "u3", "incoming", 200.0, "1/2/2023"),
                 ("u1", "u1a1", "u4", "outgoing", 150.0, "1/3/2023"),
                 ("u2", "u2a1", "u5", "outgoing", 100.0, "1/2/2023"),
                 ("u2", "u2a1", "u6", "outgoing", 120.0, "1/4/2023"),
                 ("u1", "u1a1", "u4", "incoming", 200.0, "1/5/2023")
                ]

    test_df = spark_test.createDataFrame(test_data, schema=customSchema)

    # Calling the function to calculate the transaction
    result_df = Get_Transaction(test_df)

    # Asserting the result
    expected_data = [("u1", "u4", 350.0),
                     ("u1", "u3", 300.0),
                     ("u2", "u6", 120.0),
                     ("u2", "u5", 100.0)
                    ]
    customSchema1 = StructType([
    StructField("user_id", StringType(), True),
    StructField("counterparty_id", StringType(), True),
    StructField("amount", DoubleType(), True)
])
    expected_df = spark_test.createDataFrame(expected_data, schema=customSchema1)

    assert result_df.collect() == expected_df.collect()

#Running the test
test_Get_Transaction()